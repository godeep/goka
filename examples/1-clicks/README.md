# Example 1 User Clicks

This very simple example gives an impression on how to

* Write an emitter to push data to kafka
* Write a processor that consumes data from kafka, reducing it to a user state
* Writing a view to query the user state

## How to get it running
```bash
# go to the goka examples directory
cd $GOPATH/src/github.com/lovoo/goka/examples
# make sure kafka/zookkeeper are running
make restart

# run the example
go run 1-clicks/main.go
```
Now open the browser and get the number of clicks for user-3: <http://localhost:9095/user-3>

This should return e.g.

```json
{"Clicks":153}
```

## What is it doing?
In this example, our goal is to count the number of clicks for each user.

**Emitter**

We assume users perform clicks and for each click an event is pushed to Kafka. To simulate this, we use an
`Emitter` that conveniently allows us to write objects to a specific topic. The key for each message is the user ID which is simply
generated by the sender loop.

The `Emitter` is created specifying the topic and a `Codec` that marshals passed messages for us into Kafka.
Here we use a codec provided by goka called `codec.String`, that simply marshals from and two `string` values.
In our case that's sufficient since a payload of a click is simply a time-string. If we wanted to write more
complex objects, like structs containing a link or more IDs, we would have to implement our own
codec, similar to the `userCodec` mentioned below.

**Processor**

A processor consumes those click events and reduces them by counting the number of clicks each
user has created. To achieve this, we configure the processor using `goka.DefineGroup`, which we later
pass to the processor-constructor.

```go
g := goka.DefineGroup(group,
  goka.Input(topic, new(codec.String), process),
  goka.Persist(new(userCodec)),
)
```

* `goka.Input` configures the processor to consume the topic as a stream using the `string`-codec.
The consumer of a topic must use the same codec as the writer, otherwise we'll get nasty errors or
unmarshalling will simply fail.

* `goka.Persist` makes the processor store its group table persistently using kafka. That means on every
restart (either the same host or somewhere else), the state will be restored.
This option also makes the processor cache the group table locally using a key-value store.
That avoids holding the full state in memory and a long-running recovery on every restart.

  To persist the group table, again we need a `Codec` which encodes the user for this case.
  We want to store objects of type `*user`, so we have to implement our own codec. In our example,
  the Codec simply marshals using the default go json-Marshaller.

For more information on configuring a processor using `DefineGroup`, see <??>.

The reduce-operation is performed in the `process` function passed to the processor-Constructor.
That function is called for every received message from Kafka.
Along with the message we receive a context `ctx goka.Context` which is our only way to interact
with Kafka and the group table.

```go
1  func process(ctx goka.Context, msg interface{}) {
2   var u *user
3   if val := ctx.Value(); val != nil {
4    u = val.(*user)
5   } else {
6    u = new(user)
7   }
8
9   u.Clicks++
10  ctx.SetValue(u)
11 }
```

First, we try to retrieve the value from the group table that matches the message's key (3).
If it exists, it should be a `*user`, because that's what we will store there later and what the
codec expects (4). If it's nil, the user has not been saved yet and we'll create it (6).
Now that we have a user, we simply increment the clicks and update it in our group table.

**Partitioning and Concurrency**

Messages are partitioned in Kafka using the message key. Messages within each partition are processed sequentially,
but in parallel for all partitions. That means for our case, different users can be modified
in parallel. A single user however is always modified sequentially, because all messages for that user are assigned
to the same partition which runs sequentially.

That's why there is no need to create any locks as long as all modifications are performed using the `context`.
Everything else needs to be protected by locks as usual.


**View**

For this example, we want to have a look on the user counter one by one. To query a user's click
count we use a *View* on the processor's group table and create a simple web endpoint
to query it.
In contrast to a processor, a view always contains all partitions and allows query
values of any key. Think of it as a lookup table.

The view also needs a codec for the values, as it caches the group table locally on disk like the processor does.

## Handling codec errors
Note that errors returned by the codec lead to a shutdown of the
processor/view/emitter using it immediately. We chose that fail-early-approach since data corruption would occur if,
for example, a processor accidentally reads and writes using a wrong codec and mixes different codecs in the group table.
If you need to tolerate codec-errors, you'll have to handle them inside the codec and make sure it returns a `nil`-error.
